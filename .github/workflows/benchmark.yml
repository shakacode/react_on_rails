name: Benchmark Workflow

on:
  workflow_dispatch:
    inputs:
      routes:
        description: 'Comma-separated routes to benchmark (e.g., "/,/hello"). Leave empty to auto-detect from Rails.'
        required: false
        type: string
      rate:
        description: 'Requests per second (use "max" for maximum throughput)'
        required: false
        default: 'max'
        type: string
      duration:
        description: 'Duration (e.g., "30s", "1m", "90s")'
        required: false
        default: '30s'
        type: string
      request_timeout:
        description: 'Request timeout (e.g., "60s", "1m", "90s")'
        required: false
        default: '60s'
        type: string
      connections:
        description: 'Concurrent connections/virtual users (also used as max)'
        required: false
        default: 10
        type: number
      web_concurrency:
        description: 'Number of Puma worker processes'
        required: false
        default: 4
        type: number
      rails_threads:
        description: 'Number of Puma threads (min and max will be same)'
        required: false
        default: 3
        type: number
      app_version:
        description: 'Which app version to benchmark'
        required: false
        default: 'both'
        type: choice
        options:
          - 'both'
          - 'core_only'
          - 'pro_only'
          - 'pro_rails_only'
          - 'pro_node_renderer_only'
  push:
    branches:
      - master
    paths-ignore:
      - '**.md'
      - 'docs/**'
  pull_request:
    types: [opened, synchronize, reopened, labeled]
    paths-ignore:
      - '**.md'
      - 'docs/**'
env:
  RUBY_VERSION: '3.3.7'
  BUNDLER_VERSION: '2.5.4'
  K6_VERSION: '1.4.2'
  VEGETA_VERSION: '12.13.0'
  # Determine which apps/benchmarks to run (default is 'both' for all triggers)
  RUN_CORE: ${{ contains(fromJSON('["both", "core_only"]'), github.event.inputs.app_version || 'both') && 'true' || '' }}
  RUN_PRO: ${{ (github.event.inputs.app_version || 'both') != 'core_only' && 'true' || '' }}
  RUN_PRO_RAILS: ${{ contains(fromJSON('["both", "pro_only", "pro_rails_only"]'), github.event.inputs.app_version || 'both') && 'true' || '' }}
  RUN_PRO_NODE_RENDERER: ${{ contains(fromJSON('["both", "pro_only", "pro_node_renderer_only"]'), github.event.inputs.app_version || 'both') && 'true' || '' }}
  # Benchmark parameters (defaults in bench.rb unless overridden here for CI)
  ROUTES: ${{ github.event.inputs.routes }}
  RATE: ${{ github.event.inputs.rate || 'max' }}
  DURATION: ${{ github.event.inputs.duration }}
  REQUEST_TIMEOUT: ${{ github.event.inputs.request_timeout }}
  CONNECTIONS: ${{ github.event.inputs.connections }}
  MAX_CONNECTIONS: ${{ github.event.inputs.connections }}
  # WEB_CONCURRENCY default is set dynamically to NPROC-1 in "Configure CPU pinning" step
  WEB_CONCURRENCY: ${{ github.event.inputs.web_concurrency }}
  RAILS_MAX_THREADS: ${{ github.event.inputs.rails_threads || 3 }}
  RAILS_MIN_THREADS: ${{ github.event.inputs.rails_threads || 3 }}

jobs:
  benchmark:
    # Run on: push to master, workflow_dispatch, or PRs with 'full-ci' or 'benchmark' labels
    # See https://bencher.dev/docs/how-to/github-actions/#pull-requests for the extra pull_request condition
    if: |
      github.event_name == 'push' ||
      github.event_name == 'workflow_dispatch' ||
      (
        github.event_name == 'pull_request' && github.event.pull_request.head.repo.full_name == github.repository &&
        (contains(github.event.pull_request.labels.*.name, 'full-ci') ||
         contains(github.event.pull_request.labels.*.name, 'benchmark'))
      )
    runs-on: ubuntu-latest
    permissions:
      contents: read
      issues: write
      pull-requests: write
      checks: write
    env:
      SECRET_KEY_BASE: 'dummy-secret-key-for-ci-testing-not-used-in-production'
      REACT_ON_RAILS_PRO_LICENSE: ${{ secrets.REACT_ON_RAILS_PRO_LICENSE_V2 }}

    steps:
      # ============================================
      # STEP 1: CHECKOUT CODE
      # ============================================
      - name: Checkout repository
        uses: actions/checkout@v4

      # ============================================
      # STEP 2: INSTALL BENCHMARKING TOOLS
      # ============================================

      - name: Install Bencher CLI
        uses: bencherdev/bencher@main

      - name: Add tools directory to PATH
        run: |
          mkdir -p ~/bin
          echo "$HOME/bin" >> $GITHUB_PATH

      - name: Cache Vegeta binary
        id: cache-vegeta
        if: env.RUN_PRO
        uses: actions/cache@v4
        with:
          path: ~/bin/vegeta
          key: vegeta-${{ runner.os }}-${{ runner.arch }}-${{ env.VEGETA_VERSION }}

      - name: Install Vegeta
        if: env.RUN_PRO && steps.cache-vegeta.outputs.cache-hit != 'true'
        run: |
          echo "üì¶ Installing Vegeta v${VEGETA_VERSION}"

          # Download and extract vegeta binary
          wget -q https://github.com/tsenart/vegeta/releases/download/v${VEGETA_VERSION}/vegeta_${VEGETA_VERSION}_linux_amd64.tar.gz
          tar -xzf vegeta_${VEGETA_VERSION}_linux_amd64.tar.gz

          # Store in cache directory
          mv vegeta ~/bin/

      - name: Setup k6
        uses: grafana/setup-k6-action@v1
        with:
          k6-version: ${{ env.K6_VERSION }}

      # ============================================
      # STEP 3: START APPLICATION SERVER
      # ============================================

      - name: Setup Ruby
        uses: ruby/setup-ruby@v1
        with:
          ruby-version: ${{ env.RUBY_VERSION }}
          bundler: ${{ env.BUNDLER_VERSION }}

      - name: Get gem home directory
        run: echo "GEM_HOME_PATH=$(gem env home)" >> $GITHUB_ENV

      - name: Cache foreman gem
        id: cache-foreman
        uses: actions/cache@v4
        with:
          path: ${{ env.GEM_HOME_PATH }}
          key: foreman-gem-${{ runner.os }}-ruby-${{ env.RUBY_VERSION }}

      - name: Install foreman
        if: steps.cache-foreman.outputs.cache-hit != 'true'
        run: gem install foreman

      - name: Fix dependency for libyaml-dev
        run: sudo apt install libyaml-dev -y

      # Follow https://github.com/pnpm/action-setup?tab=readme-ov-file#use-cache-to-reduce-installation-time
      - name: Setup pnpm
        uses: pnpm/action-setup@v4
        with:
          cache: true
          cache_dependency_path: '**/pnpm-lock.yaml'
          run_install: false

      - name: Setup Node
        uses: actions/setup-node@v4
        with:
          node-version: '22'

      - name: Print system information
        run: |
          echo "Linux release: "; cat /etc/issue
          echo "Current user: "; whoami
          echo "Current directory: "; pwd
          echo "Ruby version: "; ruby -v
          echo "Node version: "; node -v
          echo "Pnpm version: "; pnpm --version
          echo "Bundler version: "; bundle --version

      - name: Configure CPU pinning and process priority
        run: |
          # CPU pinning: server gets CPUs 1-(nproc-1), benchmark script gets CPU 0
          # This avoids contention and improves consistency on GitHub Actions runners
          NPROC=$(nproc)
          echo "Available CPUs: $NPROC"

          # Server: high priority (-20), pinned to CPUs 1+
          # Benchmark: elevated priority (-10), pinned to CPU 0
          SERVER_CMD="nice -n -20 taskset -c 1-$((NPROC-1)) bin/prod"
          BENCH_CMD="nice -n -10 taskset -c 0 ruby"

          echo "SERVER_CMD=$SERVER_CMD" >> $GITHUB_ENV
          echo "BENCH_CMD=$BENCH_CMD" >> $GITHUB_ENV

          # Set Puma workers to match available server CPUs (only if not explicitly set)
          if [ -z "$WEB_CONCURRENCY" ]; then
            WEB_CONCURRENCY=$((NPROC-1))
            echo "WEB_CONCURRENCY=$WEB_CONCURRENCY" >> $GITHUB_ENV
            echo "WEB_CONCURRENCY (auto): $WEB_CONCURRENCY"
          else
            echo "WEB_CONCURRENCY (from input): $WEB_CONCURRENCY"
          fi

          echo "SERVER_CMD: $SERVER_CMD"
          echo "BENCH_CMD: $BENCH_CMD"

      - name: Install Node modules with pnpm
        run: pnpm install --frozen-lockfile

      - name: Build workspace packages
        run: pnpm run build

      - name: Save Core dummy app ruby gems to cache
        if: env.RUN_CORE
        uses: actions/cache@v4
        with:
          path: react_on_rails/spec/dummy/vendor/bundle
          key: v4-core-dummy-app-gem-cache-${{ hashFiles('react_on_rails/spec/dummy/Gemfile.lock') }}

      - name: Install Ruby Gems for Core dummy app
        if: env.RUN_CORE
        run: |
          cd react_on_rails/spec/dummy
          bundle config set path vendor/bundle
          bundle config set frozen true
          bundle _${BUNDLER_VERSION}_ install --jobs=4 --retry=3

      - name: Prepare Core production assets
        if: env.RUN_CORE
        run: |
          set -e  # Exit on any error
          echo "üî® Building production assets..."
          cd react_on_rails/spec/dummy

          if ! bin/prod-assets; then
            echo "‚ùå ERROR: Failed to build production assets"
            exit 1
          fi

          echo "‚úÖ Production assets built successfully"

      - name: Start Core production server
        if: env.RUN_CORE
        run: |
          set -e  # Exit on any error
          echo "üöÄ Starting production server..."
          cd react_on_rails/spec/dummy

          $SERVER_CMD &
          echo "Server started in background"

          # Wait for server to be ready (max 30 seconds)
          echo "‚è≥ Waiting for server to be ready..."
          for i in {1..30}; do
            if curl -fsS http://localhost:3001 > /dev/null; then
              echo "‚úÖ Server is ready and responding"
              exit 0
            fi
            echo "  Attempt $i/30: Server not ready yet..."
            sleep 1
          done

          echo "‚ùå ERROR: Server failed to start within 30 seconds"
          exit 1

      # ============================================
      # STEP 4: RUN CORE BENCHMARKS
      # ============================================

      - name: Execute Core benchmark suite
        if: env.RUN_CORE
        timeout-minutes: 120
        run: |
          set -e  # Exit on any error
          echo "üèÉ Running Core benchmark suite..."

          if ! $BENCH_CMD benchmarks/bench.rb; then
            echo "‚ùå ERROR: Benchmark execution failed"
            exit 1
          fi

          echo "‚úÖ Benchmark suite completed successfully"

      - name: Validate Core benchmark results
        if: env.RUN_CORE
        run: |
          set -e
          echo "üîç Validating benchmark results..."

          if [ ! -f "bench_results/summary.txt" ]; then
            echo "‚ùå ERROR: benchmark summary file not found"
            exit 1
          fi

          echo "‚úÖ Benchmark results found"
          echo ""
          echo "üìä Summary:"
          column -t -s $'\t' bench_results/summary.txt
          echo ""
          echo "Generated files:"
          ls -lh bench_results/

      - name: Upload Core benchmark results
        uses: actions/upload-artifact@v4
        if: env.RUN_CORE && always()
        with:
          name: benchmark-core-results-${{ github.run_number }}
          path: bench_results/
          retention-days: 30
          if-no-files-found: warn

      - name: Stop Core production server
        # RUN_PRO because we only need to stop it to run the Pro server
        if: env.RUN_CORE && env.RUN_PRO && always()
        run: |
          echo "üõë Stopping Core production server..."
          # Kill all server-related processes (safe in isolated CI environment)
          pkill -9 -f "ruby|node|foreman|overmind|puma" || true

          # Wait for port 3001 to be free
          echo "‚è≥ Waiting for port 3001 to be free..."
          for _ in {1..10}; do
            if ! lsof -ti:3001 > /dev/null 2>&1; then
              echo "‚úÖ Port 3001 is now free"
              exit 0
            fi
            sleep 1
          done

          echo "‚ùå ERROR: Port 3001 is still in use after 10 seconds"
          echo "Processes using port 3001:"
          lsof -i:3001 || true
          exit 1

      # ============================================
      # STEP 5: SETUP PRO APPLICATION SERVER
      # ============================================

      - name: Cache Pro dummy app Ruby gems
        if: env.RUN_PRO
        uses: actions/cache@v4
        with:
          path: react_on_rails_pro/spec/dummy/vendor/bundle
          key: v4-pro-dummy-app-gem-cache-${{ hashFiles('react_on_rails_pro/spec/dummy/Gemfile.lock') }}

      - name: Install Ruby Gems for Pro dummy app
        if: env.RUN_PRO
        run: |
          cd react_on_rails_pro/spec/dummy
          bundle config set path vendor/bundle
          bundle config set frozen true
          bundle _${BUNDLER_VERSION}_ install --jobs=4 --retry=3

      - name: Generate file-system based entrypoints for Pro
        if: env.RUN_PRO
        run: cd react_on_rails_pro/spec/dummy && bundle exec rake react_on_rails:generate_packs

      - name: Prepare Pro production assets
        if: env.RUN_PRO
        run: |
          set -e
          echo "üî® Building Pro production assets..."
          cd react_on_rails_pro/spec/dummy

          if ! bin/prod-assets; then
            echo "‚ùå ERROR: Failed to build production assets"
            exit 1
          fi

          echo "‚úÖ Production assets built successfully"

      - name: Start Pro production server
        if: env.RUN_PRO
        run: |
          set -e
          echo "üöÄ Starting Pro production server..."
          cd react_on_rails_pro/spec/dummy

          $SERVER_CMD &
          echo "Server started in background"

          # Wait for server to be ready (max 30 seconds)
          echo "‚è≥ Waiting for server to be ready..."
          for i in {1..30}; do
            if curl -fsS http://localhost:3001 > /dev/null; then
              echo "‚úÖ Server is ready and responding"
              exit 0
            fi
            echo "  Attempt $i/30: Server not ready yet..."
            sleep 1
          done

          echo "‚ùå ERROR: Server failed to start within 30 seconds"
          exit 1

      # ============================================
      # STEP 6: RUN PRO BENCHMARKS
      # ============================================

      - name: Execute Pro benchmark suite
        if: env.RUN_PRO_RAILS
        timeout-minutes: 120
        run: |
          set -e
          echo "üèÉ Running Pro benchmark suite..."

          if ! PRO=true $BENCH_CMD benchmarks/bench.rb; then
            echo "‚ùå ERROR: Benchmark execution failed"
            exit 1
          fi

          echo "‚úÖ Benchmark suite completed successfully"

      - name: Execute Pro Node Renderer benchmark suite
        if: env.RUN_PRO_NODE_RENDERER
        timeout-minutes: 30
        run: |
          set -e
          echo "üèÉ Running Pro Node Renderer benchmark suite..."

          if ! $BENCH_CMD benchmarks/bench-node-renderer.rb; then
            echo "‚ùå ERROR: Node Renderer benchmark execution failed"
            exit 1
          fi

          echo "‚úÖ Node Renderer benchmark suite completed successfully"

      - name: Validate Pro benchmark results
        if: env.RUN_PRO
        run: |
          set -e
          echo "üîç Validating benchmark results..."

          if [ "$RUN_PRO_RAILS" = "true" ]; then
            if [ ! -f "bench_results/summary.txt" ]; then
              echo "‚ùå ERROR: Rails benchmark summary file not found"
              exit 1
            fi
            echo "üìä Rails Benchmark Summary:"
            column -t -s $'\t' bench_results/summary.txt
            echo ""
          fi

          if [ "$RUN_PRO_NODE_RENDERER" = "true" ]; then
            if [ ! -f "bench_results/node_renderer_summary.txt" ]; then
              echo "‚ùå ERROR: Node Renderer benchmark summary file not found"
              exit 1
            fi
            echo "üìä Node Renderer Benchmark Summary:"
            column -t -s $'\t' bench_results/node_renderer_summary.txt
            echo ""
          fi

          echo "‚úÖ Benchmark results validated"
          echo ""
          echo "Generated files:"
          ls -lh bench_results/

      - name: Upload Pro benchmark results
        uses: actions/upload-artifact@v4
        if: env.RUN_PRO && always()
        with:
          name: benchmark-pro-results-${{ github.run_number }}
          path: bench_results/
          retention-days: 30
          if-no-files-found: warn

      # ============================================
      # STEP 7: STORE BENCHMARK DATA
      # ============================================
      # See: https://bencher.dev/docs/how-to/track-benchmarks/ and
      #      https://bencher.dev/docs/how-to/github-actions/
      #
      # Threshold configuration using Student's t-test:
      #   Uses historical data to detect statistically significant regressions
      #   rather than a fixed percentage. The boundary value (0.95) is the
      #   confidence interval ‚Äî an alert fires when a new metric falls outside
      #   the 95% CI of the historical distribution.
      #   - rps: Lower Boundary (higher is better)
      #   - p50/p90/p99_latency_ms: Upper Boundary (lower is better)
      #   - failed_pct: Upper Boundary (lower is better)
      #
      #   max-sample-size limits historical data to the most recent 64 runs
      #   to keep the baseline relevant as performance evolves.

      - name: Track benchmarks with Bencher
        env:
          GH_TOKEN: ${{ secrets.GITHUB_TOKEN }}
        run: |
          # Confidence interval for t-test (0.95 = 95% CI)
          BOUNDARY=0.95
          MAX_SAMPLE=64

          # Set branch and start-point based on event type
          if [ "${{ github.event_name }}" = "push" ]; then
            BRANCH="master"
            START_POINT="master"
            START_POINT_HASH="${{ github.event.before }}"
            EXTRA_ARGS=""
          elif [ "${{ github.event_name }}" = "pull_request" ]; then
            BRANCH="$GITHUB_HEAD_REF"
            START_POINT="$GITHUB_BASE_REF"
            START_POINT_HASH="${{ github.event.pull_request.base.sha }}"
            EXTRA_ARGS="--start-point-reset"
          elif [ "${{ github.event_name }}" = "workflow_dispatch" ]; then
            # Get merge-base from GitHub API (avoids needing deep fetch)
            # See: https://stackoverflow.com/a/74710919
            BRANCH="${{ github.ref_name }}"
            START_POINT="master"
            START_POINT_HASH=$(gh api "repos/${{ github.repository }}/compare/master...$BRANCH" --jq '.merge_base_commit.sha' || true)

            if [ -n "$START_POINT_HASH" ]; then
              echo "Found merge-base via API: $START_POINT_HASH"
            else
              echo "‚ö†Ô∏è Could not find merge-base with master via GitHub API, continuing without it"
            fi
            EXTRA_ARGS=""
          else
            echo "‚ùå ERROR: Unexpected event type: ${{ github.event_name }}"
            exit 1
          fi

          # Run bencher and capture HTML output (stdout) while letting stderr go to logs
          # Use set +e to capture exit code without failing immediately
          set +e
          bencher run \
            --project react-on-rails-t8a9ncxo \
            --token '${{ secrets.BENCHER_API_TOKEN }}' \
            --branch "$BRANCH" \
            --start-point "$START_POINT" \
            --start-point-hash "$START_POINT_HASH" \
            --start-point-clone-thresholds \
            --testbed github-actions \
            --adapter json \
            --file bench_results/benchmark.json \
            --err \
            --quiet \
            --format html \
            --threshold-measure rps \
            --threshold-test t_test \
            --threshold-max-sample-size $MAX_SAMPLE \
            --threshold-lower-boundary $BOUNDARY \
            --threshold-upper-boundary _ \
            --threshold-measure p50_latency \
            --threshold-test t_test \
            --threshold-max-sample-size $MAX_SAMPLE \
            --threshold-lower-boundary _ \
            --threshold-upper-boundary $BOUNDARY \
            --threshold-measure p90_latency \
            --threshold-test t_test \
            --threshold-max-sample-size $MAX_SAMPLE \
            --threshold-lower-boundary _ \
            --threshold-upper-boundary $BOUNDARY \
            --threshold-measure p99_latency \
            --threshold-test t_test \
            --threshold-max-sample-size $MAX_SAMPLE \
            --threshold-lower-boundary _ \
            --threshold-upper-boundary $BOUNDARY \
            --threshold-measure failed_pct \
            --threshold-test t_test \
            --threshold-max-sample-size $MAX_SAMPLE \
            --threshold-lower-boundary _ \
            --threshold-upper-boundary $BOUNDARY \
            $EXTRA_ARGS > bench_results/bencher_report.html
          BENCHER_EXIT_CODE=$?
          set -e

          # Post report to job summary and PR comment(s) if there's HTML output
          if [ -s bench_results/bencher_report.html ]; then
            echo "üìä Adding Bencher report to job summary..."
            cat bench_results/bencher_report.html >> "$GITHUB_STEP_SUMMARY"

            if [ "${{ github.event_name }}" = "pull_request" ]; then
              echo "üìù Splitting HTML report for PR comments..."
              ruby benchmarks/split_html_report.rb bench_results/bencher_report.html bench_results/bencher_chunk

              # Delete old Bencher report comments (identified by marker)
              echo "üóëÔ∏è Deleting old Bencher report comments..."
              gh api "repos/${{ github.repository }}/issues/${{ github.event.pull_request.number }}/comments" \
                --paginate --jq '.[] | select(.body | contains("<!-- BENCHER_REPORT -->")) | .id' | \
              while read -r comment_id; do
                echo "Deleting comment $comment_id"
                gh api -X DELETE "repos/${{ github.repository }}/issues/comments/$comment_id"
              done

              # Post each chunk as a separate comment
              COMMENT_FAILED=false
              for chunk_file in bench_results/bencher_chunk*.html; do
                if [ -f "$chunk_file" ]; then
                  echo "Posting $chunk_file ($(wc -c < "$chunk_file") bytes)..."
                  if ! gh pr comment ${{ github.event.pull_request.number }} --body-file "$chunk_file"; then
                    echo "‚ö†Ô∏è Failed to post $chunk_file"
                    COMMENT_FAILED=true
                  fi
                fi
              done

              # If any chunk failed to post, add a fallback comment
              if [ "$COMMENT_FAILED" = "true" ]; then
                echo "‚ö†Ô∏è Some chunks failed to post, adding fallback comment..."
                FALLBACK_BODY="<!-- BENCHER_REPORT -->"
                FALLBACK_BODY="${FALLBACK_BODY}"$'\n'"‚ö†Ô∏è **Bencher report chunks were too large to post as PR comments.**"
                FALLBACK_BODY="${FALLBACK_BODY}"$'\n'$'\n'"View the full report in the [job summary](${{ github.server_url }}/${{ github.repository }}/actions/runs/${{ github.run_id }})."
                gh pr comment ${{ github.event.pull_request.number }} --body "$FALLBACK_BODY" || true
              fi
            fi
          else
            echo "‚úÖ No alerts - no Bencher report to post"
          fi

          # Exit with bencher's exit code to fail workflow if there was an alert
          exit $BENCHER_EXIT_CODE

      # ============================================
      # STEP 8: WORKFLOW COMPLETION
      # ============================================
      - name: Workflow summary
        if: always()
        run: |
          echo "üìã Benchmark Workflow Summary"
          echo "===================================="
          echo "Status: ${{ job.status }}"
          echo "Run number: ${{ github.run_number }}"
          echo "Triggered by: ${{ github.actor }}"
          echo "Branch: ${{ github.ref_name }}"
          echo "Run Core: ${{ env.RUN_CORE || 'false' }}"
          echo "Run Pro Rails: ${{ env.RUN_PRO_RAILS || 'false' }}"
          echo "Run Pro Node Renderer: ${{ env.RUN_PRO_NODE_RENDERER || 'false' }}"
          echo ""
          if [ "${{ job.status }}" == "success" ]; then
            echo "‚úÖ All steps completed successfully"
          else
            echo "‚ùå Workflow encountered errors - check logs above"
          fi
