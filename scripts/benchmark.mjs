import * as fs from 'fs';
import path from 'path';
import { parseArgs, colors, arraysDifference } from './utils.mjs';

const RELATIVE_ERROR_MARGING_THRESHOLD = 5; /* percentage % */

/**
 * Print usage help
 */
function printHelp() {
  console.log(`
  ${colors.blue}Compare Experiment and Control benchmarks and fail if there is a performance regression${colors.reset}

  ${colors.yellow}Usage:${colors.reset}
    node scripts/benchmark.mjs --control /tmp/control-benchmark --experiment ./benchmark

  ${colors.yellow}Options:${colors.reset}
    --control <directory>     Path to benchmark directory generated by benchmark script on the control branch
    --experiment <directory>  Path to benchmark directory generated by benchmark script on the experiment branch
  `);
}

const { experiment, control, help } = parseArgs(process.argv.slice(1));

if (help) {
  printHelp();
  process.exit(0);
}

if (!experiment || !control) {
  printHelp();
  process.exit(1);
}

const controlBenchmarkDirectoryPath = path.join(control, 'results');
const controlBenchmarks = fs.readdirSync(controlBenchmarkDirectoryPath);
if (controlBenchmarks.length === 0) {
  throw new Error(`The control benchmark directory "${controlBenchmarkDirectoryPath}" is empty`);
}

const experimentBenchmarkDirectoryPath = path.join(experiment, 'results');
const experimentBenchmarks = fs.readdirSync(experimentBenchmarkDirectoryPath);
if (experimentBenchmarks.length === 0) {
  throw new Error(`The experiment benchmark directory "${experimentBenchmarkDirectoryPath}" is empty`);
}

const benchmarksMissingFromControl = arraysDifference(experimentBenchmarks, controlBenchmarks);
if (benchmarksMissingFromControl.length > 0) {
  throw new Error(
    `The following benchmark results exists at experiment but doesn't eixst at control:\n${benchmarksMissingFromControl.map((benchmark) => `  - ${benchmark}`)}`,
  );
}

const benchmarksMissingFromExperiment = arraysDifference(controlBenchmarks, experimentBenchmarks);
if (benchmarksMissingFromExperiment.length > 0) {
  throw new Error(
    `The following benchmark results exists at control but doesn't eixst at experiment:\n${benchmarksMissingFromExperiment.map((benchmark) => `  - ${benchmark}`)}`,
  );
}

function readBenchmarks(benchmarkDirectory, benchmarkFiles) {
  return benchmarkFiles.map((benchmarkFile) => {
    const fileContent = fs.readFileSync(path.join(benchmarkDirectory, benchmarkFile));
    try {
      return {
        ...JSON.parse(fileContent),
        fileName: benchmarkFile,
      };
    } catch (err) {
      throw new Error(`Error while parsing content of the benchmark file "${benchmarkFile}: ${err}"`);
    }
  });
}

function ensureBenchmarkConfidence(benchmarksInfo, suitType) {
  let confidenceErrorMsg = '';
  benchmarksInfo.forEach((benchmarkInfo) => {
    const nonConfidentTests = benchmarkInfo.results.filter(
      (result) => result.name !== 'warm up' && result.margin > 5,
    );

    if (nonConfidentTests.length > 0) {
      const { name: suitName, fileName } = benchmarkInfo;
      confidenceErrorMsg += `The following tests at ${suitType} suit: "${suitName}" and file "${fileName}" have error margin bigger than "${RELATIVE_ERROR_MARGING_THRESHOLD}%":\n`;
      confidenceErrorMsg += nonConfidentTests.map((test) => `  - ${test.name} has "${test.margin}%"\n`);
      confidenceErrorMsg += '\n\n';
    }
  });

  if (confidenceErrorMsg.length > 0) {
    throw new Error(confidenceErrorMsg);
  }
}

const generateTestsMissingErrorMsg = (firstBenchmarkInfo, secondBenchmarkInfo, firstBenchmarkType) => {
  const firstTestsSet = firstBenchmarkInfo.results.map((t) => t.name);
  const secondTestsSet = secondBenchmarkInfo.results.map((t) => t.name);
  const missingTests = arraysDifference(firstTestsSet, secondTestsSet);
  if (missingTests.length === 0) {
    return '';
  }

  const { name, fileName } = firstBenchmarkInfo;
  return `The following tests are missing from the ${firstBenchmarkType} suit "${name}" at file "${fileName}":\n${missingTests.map(
    (t) => `  - ${t}\n`,
  )}\n\n`;
};

const controlBenchmarksInfo = readBenchmarks(controlBenchmarkDirectoryPath, controlBenchmarks, 'control');
ensureBenchmarkConfidence(controlBenchmarksInfo, 'control');
const experimentBenchmarksInfo = readBenchmarks(
  experimentBenchmarkDirectoryPath,
  experimentBenchmarks,
  'experiment',
);
ensureBenchmarkConfidence(experimentBenchmarksInfo, 'experiment');

// Ensure Same Tests Exist at Control and Experiement
const missingTestsErrorMsg = controlBenchmarksInfo.reduce((errorMsg, controlBenchmarkInfo) => {
  const experimentBenchmarkInfo = experimentBenchmarksInfo.find(
    (b) => b.fileName === controlBenchmarkInfo.fileName,
  );
  let curErrorMsg = generateTestsMissingErrorMsg(controlBenchmarkInfo, experimentBenchmarkInfo, 'experiment');
  curErrorMsg += generateTestsMissingErrorMsg(experimentBenchmarkInfo, controlBenchmarkInfo, 'control');
  return errorMsg + curErrorMsg;
}, '');

if (missingTestsErrorMsg) {
  throw new Error(missingTestsErrorMsg);
}

let hasPerformanceRegression = false;
// Compare Ops of all tests
const performanceChangeSummary = controlBenchmarksInfo.reduce(
  (performanceChangeSummary, controlBenchmarkInfo) => {
    const experimentBenchmarkInfo = experimentBenchmarksInfo.find(
      (b) => b.fileName === controlBenchmarkInfo.fileName,
    );

    const curSummary = controlBenchmarkInfo.results.reduce((testsSummary, controlTest) => {
      const experimentTest = experimentBenchmarkInfo.results.find((t) => t.name === controlTest.name);
      const controlOps = controlTest.ops;
      const experimentOps = experimentTest.ops;
      const changePercentage = ((experimentOps - controlOps) / controlOps) * 100;

      if (controlTest.name === 'warm up') {
        return testsSummary;
      }

      const changePercentageStr = Math.abs(changePercentage).toFixed(2);
      if (changePercentage < -5) {
        hasPerformanceRegression = true;
        return `${
          testsSummary
        }${colors.red}  - The test "${controlTest.name}" performance decreased by ${changePercentageStr}%${colors.reset}\n`;
      }
      if (changePercentage > 5) {
        return `${
          testsSummary
        }${colors.green}  - The test "${controlTest.name}" performance increased by ${changePercentageStr}%${colors.reset}\n`;
      }

      return `${
        testsSummary
      }  - The test "${controlTest.name}" performance slightly changed by ${changePercentageStr}%\n`;
    }, `Suit "${controlBenchmarkInfo.name}" at file "${controlBenchmarkInfo.fileName}":\n`);

    return `${performanceChangeSummary}${curSummary}\n\n`;
  },
  '',
);

console.log(performanceChangeSummary);
if (hasPerformanceRegression) {
  process.exit(1);
}
